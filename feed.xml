<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ja"><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://keisuke-yanagisawa.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://keisuke-yanagisawa.github.io/" rel="alternate" type="text/html" hreflang="ja" /><updated>2024-06-17T01:22:51+00:00</updated><id>https://keisuke-yanagisawa.github.io/feed.xml</id><title type="html">柳澤 渓甫 | Keisuke Yanagisawa</title><subtitle>An assistant professor in Tokyo Tech.</subtitle><author><name>Keisuke Yanagisawa</name><email>yanagisawa@c.titech.ac.jp</email></author><entry><title type="html">東京都のCOVID-19モニタリングデータ(1)</title><link href="https://keisuke-yanagisawa.github.io/202202/temporal_data_tokyo_covid_1/" rel="alternate" type="text/html" title="東京都のCOVID-19モニタリングデータ(1)" /><published>2022-02-26T00:00:00+00:00</published><updated>2022-02-26T00:00:00+00:00</updated><id>https://keisuke-yanagisawa.github.io/202202/temporal_data_tokyo_covid_1</id><content type="html" xml:base="https://keisuke-yanagisawa.github.io/202202/temporal_data_tokyo_covid_1/"><![CDATA[<p>画像の識別など、数か月程度で考えれば時系列を気にしなくて済むデータと比べて、時系列データは考慮すべきことが多い。
ここでは、その1つの例として、<a href="https://stopcovid19.metro.tokyo.lg.jp/monitoring/">東京都の新型コロナウイルス感染症対策サイト</a>
からデータを取得し、少しばかり、実際の作業をやってみようと思う。</p>

<h2 id="何をするか決める">何をするか決める</h2>

<p>まず、ここで何をやるかを決めておこう。
今回は、 <strong>東京都の翌日の新規陽性者数を予測する</strong> ことをやってみる<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>。</p>

<h2 id="データを取得する">データを取得する</h2>

<p>ホームページにアクセスすると、さまざまなモニタリング項目が表示され、
その左下に小さく オープンデータを入手 というリンクが存在している。
ここでは、 <strong>モニタリング項目(1) 新規陽性者数</strong> を対象に作業を行なう。
当該のリンクをクリックすると、<a href="https://catalog.data.metro.tokyo.lg.jp/dataset/t000010d0000000068">このページ</a>に遷移する。</p>

<p>ここから <code class="language-plaintext highlighter-rouge">130001_tokyo_covid19_patients.csv</code> 取得する。</p>

<p>このデータの中身をpandasで見てみよう。（Excelで開くにはサイズが大きすぎる、何故かはすぐにわかる）</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"130001_tokyo_covid19_patients.csv"</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="s">"No"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span> <span class="c1"># Jupyterでは df.head(5) の方が綺麗に描画される
</span><span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">))</span> <span class="c1"># =&gt; 962,673
</span></code></pre></div></div>

<p><img src="https://keisuke-yanagisawa.github.io/assets/posts/2022/img/2022-02-26_01.png" alt="データの中身の一例" /></p>

<p>このデータを見ると、日々の件数ではなく、それぞれの陽性者の情報が記載されていることがわかる。
2022年2月25日現在、東京都の陽性者数の累計は95万人を超えているので、
おのずからこのcsvファイルも95万行以上のデータを持っている。</p>

<p>今回は新規陽性者数を予測したいだけなので、
個々の人間の情報は含まずに、<strong>公表_年月日</strong> ごとに、
何人の新規陽性者がでたのかをまとめる。</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">describe</span><span class="p">())</span> <span class="c1"># =&gt; 全国地方公共団体コード が len(df) と同じcountになっている = NaNが存在しない
</span>
<span class="n">new_patients</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">"公表_年月日"</span><span class="p">).</span><span class="n">count</span><span class="p">().</span><span class="n">reset_index</span><span class="p">()</span>
<span class="n">new_patients</span><span class="p">[</span><span class="s">"新規陽性者数"</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_patients</span><span class="p">[</span><span class="s">"全国地方公共団体コード"</span><span class="p">]</span>
<span class="n">new_patients</span> <span class="o">=</span> <span class="n">new_patients</span><span class="p">[[</span><span class="s">"公表_年月日"</span><span class="p">,</span> <span class="s">"新規陽性者数"</span><span class="p">]]</span>
<span class="n">new_patients</span><span class="p">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s">"new_patients_tokyo.csv"</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="https://keisuke-yanagisawa.github.io/assets/posts/2022/img/2022-02-26_02.png" alt="new_patients" /></p>

<p>とりあえず、データは準備できた。
<a href="https://keisuke-yanagisawa.github.io/assets/posts/2022/data/2022-02-26_new_patients_tokyo.csv">ここで作成したデータを添付しておく</a>。</p>

<p>次回は、このデータの性質を考えて、どのような学習を組みたてるかを考えていこう。</p>

<!-- ## データの性質を考える

さて、これから **東京都の翌日の新規陽性者数を予測する** ことを行なっていくのだが、
予測する上で重要と思われる要素を考えてみよう。

- **曜日・祝日**：新規陽性者数のグラフを見ると、明らかに7日周期の波が存在している。また、祝日が挟まると不規則な挙動を示していることがわかる。
- **最近の**新規陽性者数：新規陽性者数は5人から突然10,000人になることはあり得ない。最近（例えば、ここ1,2週間）の新規陽性者数に影響される^[2]。
- 新規陽性者数の**増加率**：1人が複数人に感染を広げる、という構造になっている。例えば1人が平均2人に感染させるのであれば、100人いたら平均200人に感染させるはずである。この時、人数の変化を見るよりも、人数変化の「割合」を見ることが重要そうである。

[2]:逆に、遠い昔（数か月前など）の新規陽性者数は使い物にならない可能性が高い。

### 目的変数

特に、**増加率**が重要である、という事実は、予測モデルの**目的変数（予測する値）を何にするか**に影響する。

1. 新規陽性者数
2. 新規陽性者数**の対数値**
3. 新規陽性者数の1週間前比

様々考えられるが、今回は **3. 新規陽性者数の1週間前比** を目的変数としよう^[3]。

[3]:最近の1日の新規陽性者数が0人ということは考えられないため、単に比を取ることにする。0人を考慮に含める場合には分母分子にそれぞれ1を足すと良い。

### 説明変数

一方、説明変数はどうだろうか。最初に列挙したうちの **曜日・祝日** および **最近の新規陽性者数** が重要そうである。
ただ、祝日を取得するのは面倒なため、ここでは最近2週間の新規陽性者数の1週間前比（目的変数にあわせておく）とその日の曜日を説明変数としよう。
あわせて、予測したい翌日の曜日も説明変数に加えておく^[4]。

[4]:予測したい日の情報は説明変数に加えないことが原則なのだが、**翌日の曜日は容易に推定できる**ので追加している。曜日が不規則変化するとは思えない。


## データセットを作成する

それではようやく、教師あり学習に近づいていこう。
前節で決めた説明変数および目的変数に従って、データセットを作成する。

今回は、2021年8月1日以降を目的変数とするデータセットを作成することにする。


```py
import numpy as np
import pandas as pd

new_patients = pd.read_csv("new_patients_tokyo.csv")
new_patients["公表_年月日"] = pd.to_datetime(new_patients["公表_年月日"])
new_patients["曜日"] = new_patients["公表_年月日"].dt.dayofweek
new_patients.index = new_patients["公表_年月日"].view(int)//1000000000//86400 # nano sec. => day

date_from = np.datetime64("2021-08-01").astype(int) # convert to sec. (not nano sec.)
date_to = new_patients.index.max()
# print(date_from, date_to) => (18840, 19048)

new_patients.loc[date_from-14:date_to, "新規陽性者数_1週間前比"] = \
  new_patients.loc[date_from-14:date_to,"新規陽性者数"].to_numpy() / new_patients.loc[date_from-14-7:date_to-7,"新規陽性者数"].to_numpy()
# to_numpy() is needed because avoid index-based element division


dataset = []
for i in range(date_from, date_to+1):
  datapoint = [i]
  for j in range(15):
    row = new_patients.loc[i-j]
    datapoint.append(row["新規陽性者数_1週間前比"])
    # day of week should be one-hot encoded
    onehot = np.eye(7)[row["曜日"]]
    datapoint.extend(onehot)
  dataset.append(datapoint)

columns = ["date"]
for j in range(15):
  columns.extend([
    f"{j}_ratio",
    f"{j}_mon",
    f"{j}_tue",
    f"{j}_wed",
    f"{j}_thu",
    f"{j}_fri",
    f"{j}_sat",
    f"{j}_sun",
  ])

df_dataset = pd.DataFrame(dataset, columns=columns)
df_dataset.to_csv("dataset.csv", index=None)
```

この `dataset.csv` も[ここに置いておく](https://keisuke-yanagisawa.github.io/assets/posts/2022/data/2022-02-26_dataset.csv)。

## 時系列的な訓練と予測

最後に予測を行なうのだが、経時変化により、徐々にデータが集まるのが時系列データの仕組みである。
このため、例えば2021年9月1日の予測を行なう際には2021年8月31日の30件程度のデータしか訓練に使えず、
一方で2022年2月1日の予測を行なう際には180件程度のデータを訓練に利用することができる。

ここでは、以下のように訓練、検証、テストデータを分割することで、各々のステップで最良のハイパーパラメータを決定することにしよう。

1. 予測したい日付 `d` を1つ定め、これをテストデータとする（例えば、2021年10月1日）。
2. `d-7` から `d-1` の7日間を検証データとする。
3. `d-8` 以前のデータを全て訓練データとする。

データは2021年8月1日からしか準備していないので、`d` は2021年9月1日以降のみに限ることにする。

モデル自体はRidge回帰を用いることにし、 $$\alpha = {2^{-20}, 2^{-19}, ..., 2^8, 2^9}$$ を
最適化対象ハイパーパラメータとして、1日経過する毎にハイパーパラメータ探索を行なうことにする^[5]。



[5]:ハイパーパラメータ探索をどのレベルで行なうか？は検討の余地がある。すなわち、毎回ハイパーパラメータ探索を行なうと計算時間が膨大になってしまう場合には、平均的に良い予測モデルを構築できるハイパーパラメータを1つ決定し、それを今後の予測モデル構築に応用し続ける必要がある。一方、毎回ハイパーパラメータ探索を行なっても訓練に要する時間が限定的なのであれば、毎回より良いハイパーパラメータを追い求め続けるのが良いだろう。
 -->
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>本来は<strong>解析の目的</strong>があって、その目的に沿い、かつ予測が可能と思われる対象を決めるというステップを踏む必要がある。 <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Keisuke Yanagisawa</name><email>yanagisawa@c.titech.ac.jp</email></author><category term="データサイエンス・機械学習" /><category term="教師あり学習" /><category term="時系列データ" /><summary type="html"><![CDATA[画像の識別など、数か月程度で考えれば時系列を気にしなくて済むデータと比べて、時系列データは考慮すべきことが多い。 ここでは、その1つの例として、東京都の新型コロナウイルス感染症対策サイト からデータを取得し、少しばかり、実際の作業をやってみようと思う。]]></summary></entry><entry><title type="html">ブログを書き始めた理由</title><link href="https://keisuke-yanagisawa.github.io/202202/why-i-started-blog/" rel="alternate" type="text/html" title="ブログを書き始めた理由" /><published>2022-02-16T00:00:00+00:00</published><updated>2022-02-16T00:00:00+00:00</updated><id>https://keisuke-yanagisawa.github.io/202202/why-i-started-blog</id><content type="html" xml:base="https://keisuke-yanagisawa.github.io/202202/why-i-started-blog/"><![CDATA[<p>ふとした思いつきで、ブログを書けるようにした
（そのためにホームページの見た目を全面更新することになったのだが）。</p>

<p>研究者とは、企業名ではなく、個人名で勝負する仕事だ。
この人がどういう人で、どういうことを考えているか？は、
万が一私を雇用してくださる機関があったとしたら、求められる情報だろう。
そのような事を考えると、はてなブログやQiitaなどでは、
「私である」という情報が足りないと感じた。
個人の顔と、現在のポジションとあわせて読めることが重要だろう。</p>

<p>また、自身の研究論文紹介を記述する上で、英語で表現すべきことも多い。
そのような場合に、日本のサービスである はてブロ や Qiita といったものに頼るのは
世界の研究者のアクセスを鈍らせ、大きなマイナスになる。
今はまだ英語のblogを立ち上げているわけではないのだが、
自分がもっと世界に、1人でも多くに、貢献するためには
絶対に必要だと確信している。</p>]]></content><author><name>Keisuke Yanagisawa</name><email>yanagisawa@c.titech.ac.jp</email></author><category term="その他" /><category term="読み物" /><summary type="html"><![CDATA[ふとした思いつきで、ブログを書けるようにした （そのためにホームページの見た目を全面更新することになったのだが）。]]></summary></entry><entry><title type="html">検証誤差と汎化誤差</title><link href="https://keisuke-yanagisawa.github.io/202202/validation_and_generalization_error/" rel="alternate" type="text/html" title="検証誤差と汎化誤差" /><published>2022-02-13T00:00:00+00:00</published><updated>2022-02-13T00:00:00+00:00</updated><id>https://keisuke-yanagisawa.github.io/202202/validation_and_generalization_error</id><content type="html" xml:base="https://keisuke-yanagisawa.github.io/202202/validation_and_generalization_error/"><![CDATA[<p>機械学習では、train-valid-test分割 <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> という方法が良く用いられる。</p>

<p>訓練データとテストデータを分けるのは良くわかる（我々は良く「カンニング」と表現するが、これから予測したい対象を使って予測モデルを構築したらうまく予測できるのはほぼ自明である）。しかし、検証データを用いる必要性はちょっとわからないかもしれない。
この分割が何故必要なのか、考えてみようと思う。</p>

<h2 id="イメージを持つ代表選抜と本番での実力">イメージを持つ：代表選抜と本番での実力</h2>

<p>機械学習の議論をする前に、ちょっとイメージを持っておこう。</p>

<p>********</p>

<p>100m走の選手選抜を考えてみよう。とある高校で、以下のような実力伯仲な5人の100m走者がいるとする。彼らは調子によって <strong>\(\pm0.5\) 秒のタイムのぶれ</strong>がある。</p>

<table>
  <thead>
    <tr>
      <th>走者</th>
      <th style="text-align: right">平均タイム</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>山田</td>
      <td style="text-align: right">11.20 秒</td>
    </tr>
    <tr>
      <td>佐藤</td>
      <td style="text-align: right">11.21 秒</td>
    </tr>
    <tr>
      <td>田中</td>
      <td style="text-align: right">11.17 秒</td>
    </tr>
    <tr>
      <td>橋本</td>
      <td style="text-align: right">11.13 秒</td>
    </tr>
    <tr>
      <td>加藤</td>
      <td style="text-align: right">11.15 秒</td>
    </tr>
  </tbody>
</table>

<p>この5名から高校の代表選手を1名決めるために、タイム計測を行うことを考えてみよう。</p>

<p>********</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 毎回同じ結果が得られるように固定
</span>
<span class="c1"># 表の情報を記入
</span><span class="n">name</span> <span class="o">=</span> <span class="p">[</span><span class="s">"山田"</span><span class="p">,</span> <span class="s">"佐藤"</span><span class="p">,</span> <span class="s">"田中"</span><span class="p">,</span> <span class="s">"橋本"</span><span class="p">,</span> <span class="s">"加藤"</span><span class="p">]</span>
<span class="n">ave_time</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">11.20</span><span class="p">,</span> <span class="mf">11.21</span><span class="p">,</span> <span class="mf">11.17</span><span class="p">,</span> <span class="mf">11.13</span><span class="p">,</span> <span class="mf">11.15</span><span class="p">])</span>

<span class="c1"># +-0.5秒のぶれの設定（一様分布を仮定）
</span><span class="n">noise</span> <span class="o">=</span> <span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random_sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span> 

<span class="n">time</span> <span class="o">=</span> <span class="n">ave_time</span> <span class="o">+</span> <span class="n">noise</span>
<span class="n">time</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>        <span class="c1"># 小数点以下3桁までにする
</span><span class="k">print</span><span class="p">(</span><span class="s">"今回のタイム計測の記録"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">time</span><span class="p">)</span>
</code></pre></div></div>

<p>その結果、加藤くんが11.074秒で最も早く、代表選手として選ばれた。</p>

<p>さて、翌週に地区大会があったとしよう。ここで加藤くんはどのような結果を出すだろうか？ <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">ave_time</span> <span class="o">=</span> <span class="mf">11.15</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random_sample</span><span class="p">()</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">time</span> <span class="o">=</span> <span class="n">ave_time</span> <span class="o">+</span> <span class="n">noise</span>
<span class="n">time</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"地区大会での加藤くんの記録"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">time</span><span class="p">)</span>
</code></pre></div></div>

<p>これを実行してみると、代表選手として選ばれた時のタイム（11.074秒）に比べて悪いタイム（11.199秒）となってしまった。これは偶然だろうか？「たまたま調子が悪かっただけなのでは？」という気もする。</p>

<p>しかし、地区大会は一発勝負だ。調子が悪かったのかもしれなくとも、2度以上試すことはできない。そこで、100この世界線があり、それぞれの世界線で加藤くんが地区大会で走ったと考えて <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> 、そのタイムの平均を取ってみよう。</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">ave_time</span> <span class="o">=</span> <span class="mf">11.15</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mi">1</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random_sample</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.5</span>
<span class="n">time</span> <span class="o">=</span> <span class="n">ave_time</span> <span class="o">+</span> <span class="n">noise</span>
<span class="n">ave_time</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">time</span><span class="p">)</span>

<span class="n">ave_time</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">ave_time</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"地区大会での加藤くんの記録の平均"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ave_time</span><span class="p">)</span>
</code></pre></div></div>

<p>結果は平均11.123秒となり、依然代表選手として選ばれた時のタイム（11.074秒）と比べると遅い。
よくよく考えると、これはアタリマエの話である。100回、1000回と考えれば、加藤くんの記録の平均は11.150秒に収束するはずだ。この11.150秒という記録は、11.074秒に比べれば遅い。</p>

<p>大会に向けての調整をしない、という仮定を置いて考えているので現実世界とは異なる部分もあるが、<strong>選考の際に実力を十分に発揮できた人が選抜されやすい</strong>、ということが分かったと思う。</p>

<h2 id="機械学習と100m走">機械学習と100m走</h2>

<p>なぜこのような100m走の話をしたのだろうか。これは、機械学習のモデル選択と全く同じ状況だからである。</p>

<p>手元に5つの機械学習手法 + ハイパーパラメータの組があるとしよう。 <strong>本当は汎化誤差を知ることは不可能</strong>であり、全知全能な「天の声」視点に立っていると考えてほしいので、<strong>わざとカッコ書きにしている</strong>。</p>

<table>
  <thead>
    <tr>
      <th>機械学習手法 + ハイパーパラメータ</th>
      <th style="text-align: right">汎化誤差 (RMSE)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">DecisionTree(max_depth=20)</code></td>
      <td style="text-align: right">(2.958)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">DecisionTree(max_depth=5)</code></td>
      <td style="text-align: right">(2.921)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">RandomForest(n_estimators=100)</code></td>
      <td style="text-align: right">(2.863)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">RandomForest(n_estimators=200)</code></td>
      <td style="text-align: right">(2.857)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">RandomForest(n_estimators=500)</code></td>
      <td style="text-align: right">(2.854)</td>
    </tr>
  </tbody>
</table>

<p>それぞれのモデルは、性能評価に用いるデータセットの中身によって、 <strong>\(\pm0.2\) のRMSEのぶれが存在する</strong>としよう。</p>

<p>検証データで行いたいのは、この5つのモデルから、汎化誤差が最も良さそうなモデルを1つ選択することである。やってみよう。</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 毎回同じ結果が得られるように固定
</span>
<span class="c1"># 表の情報を記入
</span><span class="n">name</span> <span class="o">=</span> <span class="p">[</span><span class="s">"DT20"</span><span class="p">,</span> <span class="s">"DT5"</span><span class="p">,</span> <span class="s">"RF100"</span><span class="p">,</span> <span class="s">"RF200"</span><span class="p">,</span> <span class="s">"RF500"</span><span class="p">]</span>
<span class="n">ave_gen_err</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.958</span><span class="p">,</span> <span class="mf">2.921</span><span class="p">,</span> <span class="mf">2.863</span><span class="p">,</span> <span class="mf">2.857</span><span class="p">,</span> <span class="mf">2.854</span><span class="p">])</span>

<span class="c1"># +-0.2のぶれの設定（一様分布を仮定）
</span><span class="n">noise</span> <span class="o">=</span> <span class="mf">0.4</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random_sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.2</span> 

<span class="n">valid_err</span> <span class="o">=</span> <span class="n">ave_gen_err</span> <span class="o">+</span> <span class="n">noise</span>
<span class="n">valid_err</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">valid_err</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>        <span class="c1"># 小数点以下3桁までにする
</span><span class="k">print</span><span class="p">(</span><span class="s">"検証データを用いたときのRMSE"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">valid_err</span><span class="p">)</span>
</code></pre></div></div>

<p>この結果、<code class="language-plaintext highlighter-rouge">RandomForest(n_estimators=500)</code> が RMSE=2.823 で最も良いモデルである、と選択される。</p>

<p>さて、このモデルを本番環境に適用したときの<strong>誤差の期待値（＝汎化誤差）を知りたい</strong>、と考えてみよう。<strong>モデル選択を行った際のRMSE値を使えばいいじゃないか、と思うかもしれないが、これは正しい推定値になっているのだろうか？</strong></p>

<p><strong>誤差の期待値</strong>とは、先ほどの例における「地区大会における平均タイム」と同じであるから、独立した試行を十分な回数行った場合の平均性能を見てみればよい。</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">ave_gen_err</span> <span class="o">=</span> <span class="mf">2.854</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">0.4</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">random_sample</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.2</span> 
<span class="n">test_err</span> <span class="o">=</span> <span class="n">ave_gen_err</span> <span class="o">+</span> <span class="n">noise</span>
<span class="n">ave_test_err</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_err</span><span class="p">)</span>
<span class="n">ave_test_err</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">ave_test_err</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"誤差の期待値（汎化性能の期待値）"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">ave_test_err</span><span class="p">)</span>
</code></pre></div></div>

<p>この結果、 RMSE=2.843 となり、やはりモデル選択を行った際のRMSE (2.823) の方が良い値となっている。</p>

<p>このように、<strong>モデル選択を行った際の誤差の値というのは、期待値よりも良くなってしまい、汎化性能を正しく推定できていない。</strong>以上のことから、モデル選択を行う検証データセットのみで汎化性能を評価することは良くなく、検証データとは別に、テストデータを準備して、<strong>テストデータをもとに汎化性能を評価する</strong>必要がある。これが、train-validではなくtrain-valid-test分割を行う必要がある原因である。</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>例えば、https://towardsdatascience.com/how-to-split-data-into-three-sets-train-validation-and-test-and-why-e50d22d3e54c に記述がある <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>ここでは、簡単のために、「加藤くんは校内選抜と地区大会で同じ能力を発揮する」と仮定している。 <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>無茶苦茶な話だが…。 <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Keisuke Yanagisawa</name><email>yanagisawa@c.titech.ac.jp</email></author><category term="データサイエンス・機械学習" /><category term="教師あり学習" /><category term="予測誤差" /><summary type="html"><![CDATA[機械学習では、train-valid-test分割 1 という方法が良く用いられる。 例えば、https://towardsdatascience.com/how-to-split-data-into-three-sets-train-validation-and-test-and-why-e50d22d3e54c に記述がある &#8617;]]></summary></entry><entry><title type="html">不均衡なデータ</title><link href="https://keisuke-yanagisawa.github.io/202202/imbalanced-data/" rel="alternate" type="text/html" title="不均衡なデータ" /><published>2022-02-09T00:00:00+00:00</published><updated>2022-02-09T00:00:00+00:00</updated><id>https://keisuke-yanagisawa.github.io/202202/imbalanced-data</id><content type="html" xml:base="https://keisuke-yanagisawa.github.io/202202/imbalanced-data/"><![CDATA[<p>Bioinformaticsの分野をやっていると、<strong>不均衡なデータ</strong>によく出くわすものである。
特に、正例 positive が少なく、負例 negative が多いケースが多い。
このような状態だと、何も考えずに構築したモデルは、いかなるデータが来ようとも負例として予測してしまうことすらある。</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># 不均衡データ（1対50）を作成
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="p">[</span><span class="mi">500</span><span class="p">,</span><span class="mi">10</span><span class="p">],</span> <span class="n">centers</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">0</span><span class="p">].</span><span class="n">T</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"negative"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">y</span><span class="o">==</span><span class="mi">1</span><span class="p">].</span><span class="n">T</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"positive"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="https://keisuke-yanagisawa.github.io/assets/img/posts/202202/2022-02-09-imbalanced-data-01.png" alt="データの散布図" /></p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>

<span class="c1"># 予測モデルの構築
# （説明のためにgammaを下げて問題を誘発している）
</span><span class="n">svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">svc</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">svc</span><span class="p">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
</code></pre></div></div>

<p>これを行うと、<code class="language-plaintext highlighter-rouge">[3, -1]</code> は<strong>負例である</strong>、という予測結果が出てくる。
しかし、これを先ほど示した図に載せるとどこになるだろうか。
以下のようになり、<strong>明らかに正例であるべき場所</strong>である。</p>

<p><img src="https://keisuke-yanagisawa.github.io/assets/img/posts/202202/2022-02-09-imbalanced-data-02.png" alt="不均衡データの学習" /></p>

<p>これは意図的に変数 \(\gamma\) を下げることで（モデルの複雑度を下げて）誘発しているが、単なる正解率に基づくハイパーパラメータ探索は<strong>いつのまにか</strong>この問題に入りこんでしまう事がある。注意しなければならない。</p>

<h5 id="関連参考文献順不同">関連参考文献（順不同）</h5>
<ul>
  <li>Alice Zheng, Amanda Casari 著、株式会社ホクソエム 訳『機械学習のための特徴量エンジニアリング』（オライリージャパン、2019年）
    <ul>
      <li>不均衡データに対して、ダウンサンプリングを行うことで不均衡を是正する方法を4.2.1節で説明している。</li>
    </ul>
  </li>
  <li>中山浩太郎 監修、塚本邦尊、山田典一、大澤文孝 著『東京大学のデータサイエンティスト育成講座』（マイナビ出版、2019年）
    <ul>
      <li>不均衡データに対する予測モデルの評価において、正解率を使うのではなくROC曲線のAUC (Area Under the Curve) を使うことを10-3-2-4節で述べている。</li>
    </ul>
  </li>
  <li>門脇大輔、阪田隆司、保坂桂佑、平松雄司 著『Kaggleで勝つデータ分析の技術』（技術評論社、2019年）
    <ul>
      <li>99.4%が負例であったKaggleのコンペについて2.6.5節で触れている。ここではROC曲線ではなくPR曲線のAUCを使う話をしている。</li>
    </ul>
  </li>
</ul>]]></content><author><name>Keisuke Yanagisawa</name><email>yanagisawa@c.titech.ac.jp</email></author><category term="データサイエンス・機械学習" /><category term="教師あり学習" /><category term="データセット" /><summary type="html"><![CDATA[Bioinformaticsの分野をやっていると、不均衡なデータによく出くわすものである。 特に、正例 positive が少なく、負例 negative が多いケースが多い。 このような状態だと、何も考えずに構築したモデルは、いかなるデータが来ようとも負例として予測してしまうことすらある。]]></summary></entry><entry><title type="html">共溶媒分子動力学 (MSMD) 法における共溶媒セットの構築手法 EXPRORER</title><link href="https://keisuke-yanagisawa.github.io/research/exprorer" rel="alternate" type="text/html" title="共溶媒分子動力学 (MSMD) 法における共溶媒セットの構築手法 EXPRORER" /><published>2022-02-06T00:00:00+00:00</published><updated>2022-02-06T00:00:00+00:00</updated><id>https://keisuke-yanagisawa.github.io/research/exprorer</id><content type="html" xml:base="https://keisuke-yanagisawa.github.io/research/exprorer"><![CDATA[<p>2021年6月に <em>Journal of Chemical Information and Modeling</em> 誌に掲載された、
EXPRORER (EXtended PRObes set construction by REpresentative Retrieval) の話。</p>

<p><img src="https://keisuke-yanagisawa.github.io/assets/img/posts/202202/2022-02-06_01.png" alt="EXPRORER" /></p>

<h2 id="研究背景">研究背景</h2>

<p>共溶媒分子動力学 (mixed-solvent molecular dynamics; MSMD) 法は、
タンパク質を溶質、水分子と共溶媒分子を溶媒とした分子動力学 (molecular dynamics; MD) 法である。
共溶媒（プローブ分子）として、芳香環や疎水基、正負に帯電している分子などを用いることで、
水分子では観測できないタンパク質表面のホットスポットの発見や、
低分子結合部位の検出、低分子結合親和性の予測など、
薬剤開発の様々なステップで活用することが出来る。</p>

<p>既存の MSMD 手法である MixMD, SILCS, MDmix などは、結合親和性の評価精度改善に注力する一方、
どのような共溶媒を用いるか、のコンセンサスが取れていない。
また、分子構造は極めて多様であるが、その多様性に比べると余りにも少ないのが現状である。</p>

<h2 id="研究成果">研究成果</h2>

<p>このような問題に対して、薬剤分子に頻出する部分構造を共溶媒として切り出し、
これに対するMSMDシミュレーションを行うことで、
<strong>「網羅的」な共溶媒セットの構築を行う EXPRORER を開発した</strong><sup id="fnref:EXPRORER" role="doc-noteref"><a href="#fn:EXPRORER" class="footnote" rel="footnote">1</a></sup>。</p>

<p>前述のように、この手法は「セットの構築」が主要な目的ではあるが、
他方、MSMDシミュレーションのためのシミュレーション空間の構築や
MSMDシミュレーションの実施をフリーソフトウェアのみで完結する
仕組みを<a href="https://github.com/keisuke-yanagisawa/exprorer">github上で公開</a>している。</p>

<h2 id="雑談">（雑談）</h2>

<p>…作者本人もEXPRORERの立ち位置が、「MSMDシミュレーションツールそのもの」なのか迷う時があるが、
この手法はあくまで<strong>「共溶媒セットの構築」のための手法</strong>であることに注意したい。</p>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:EXPRORER" role="doc-endnote">
      <p><strong>Keisuke Yanagisawa</strong>, Yoshitaka Moriwaki, Tohru Terada, Kentaro Shimizu. “EXPRORER: Rational Cosolvent Set Construction Method for Cosolvent Molecular Dynamics Using Large-Scale Computation”, <em>Journal of Chemical Information and Modeling</em>, <strong>61</strong>: 2744-2753, 2021/06. DOI: <a href="https://doi.org/10.1021/acs.jcim.1c00134">10.1021/acs.jcim.1c00134</a> <a href="#fnref:EXPRORER" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Keisuke Yanagisawa</name><email>yanagisawa@c.titech.ac.jp</email></author><category term="研究成果" /><category term="共溶媒分子動力学法" /><category term="分子動力学法" /><category term="EXPRORER" /><summary type="html"><![CDATA[2021年6月に Journal of Chemical Information and Modeling 誌に掲載された、 EXPRORER (EXtended PRObes set construction by REpresentative Retrieval) の話。]]></summary></entry><entry><title type="html">内挿と外挿</title><link href="https://keisuke-yanagisawa.github.io/202202/interpolation_and_extrapolation/" rel="alternate" type="text/html" title="内挿と外挿" /><published>2022-02-02T00:00:00+00:00</published><updated>2022-02-02T00:00:00+00:00</updated><id>https://keisuke-yanagisawa.github.io/202202/interpolation_and_extrapolation</id><content type="html" xml:base="https://keisuke-yanagisawa.github.io/202202/interpolation_and_extrapolation/"><![CDATA[<p>機械学習モデルを構築する上で、
本来データが存在するはずなのにサンプリングできていない（データを取得できていない）空間があると、
その部分の予測精度は落ちてしまう。これについて少し触れてみたい。</p>

<h2 id="データの疎密と誤差の大小">データの疎密と誤差の大小</h2>

<p>とりあえず実験してみよう。ここでは、\(x\) が \([-5,5]\) の範囲における \(\cos x\) を使ってモデルを構築し、
\(x\) が \([-10,10]\) の範囲の予測を行ってみている。</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># データセット作成
</span><span class="n">train_X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">train_X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">test_X</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">test_y</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">test_X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># 予測モデルの構築
</span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
<span class="n">svr</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">()</span>
<span class="n">svr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>

<span class="c1"># テストデータに対する予測結果の描画
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"train dataset"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"green"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"true"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">svr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">"predicted"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">"lower right"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>以下に示したような図が作成されたはずだ。
この図からわかるように、訓練データ（緑点）がある区間は予測誤差は小さい一方、
訓練データが無い区間（両端）は予測誤差が大きくなっている。</p>

<p><img src="https://keisuke-yanagisawa.github.io/assets/img/posts/202202/2022-02-02_01.png" alt="予測結果" /></p>

<p>冒頭に述べたように、<strong>訓練データに存在していない領域の予測精度は低下してしまう</strong>ことがわかる。</p>

<h2 id="内挿と外挿">内挿と外挿</h2>

<p>上記で示した例は外挿、すなわち、訓練データの外側（すなわち外挿 extrapolation）の予測を行った。
一方で、学習データの内側に穴があいてしまうこともあるかもしれない。これに対する予測は「内挿 interpolation」と言える。</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># データセット作成
</span><span class="n">train_X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">(</span> 
  <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)]</span> 
<span class="p">)[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">train_X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>
<span class="n">test_X</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)[:,</span> <span class="n">np</span><span class="p">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">test_y</span>  <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">test_X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># 予測モデルの構築
</span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
<span class="n">svr</span> <span class="o">=</span> <span class="n">SVR</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="p">.</span><span class="mi">4</span><span class="p">)</span>
<span class="n">svr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>

<span class="c1"># テストデータに対する予測結果の描画
</span><span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"train dataset"</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">"green"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"true"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">svr</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s">"predicted"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">"upper right"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p>この結果を見てみると、データが中抜けしている（内挿領域である） \([-1, 1]\) の領域の予測精度はそこまで悪くなく、
一方で外挿領域である \((-\infty, -2]\) や \([2, \infty)\) では予測が大幅に間違っている。</p>

<p><img src="https://keisuke-yanagisawa.github.io/assets/img/posts/202202/2022-02-02_02.png" alt="予測結果" /></p>

<p>この結果は人間の感覚と合致している。訓練データの並びを見たとき、内挿区間をなめらかにつなごうとすれば自然と山を作る。
一方外挿区間は、この訓練データの並びが \(\cos x\) から来ているのか \(ax^2 + bx + c\) から来ているのかわからないので、
人によって異なる曲線を描くだろう。</p>

<p>なんにせよ、<strong>外挿は内挿に比べて、さらに予測精度が悪化する</strong>事があるので注意したい。</p>

<!-- ##### 参考文献
- 江崎貴裕『分析者のためのデータ解釈学入門　データの本質をとらえる技術』（ソシム、2020年） -->]]></content><author><name>Keisuke Yanagisawa</name><email>yanagisawa@c.titech.ac.jp</email></author><category term="データサイエンス・機械学習" /><category term="教師あり学習" /><category term="予測誤差" /><summary type="html"><![CDATA[機械学習モデルを構築する上で、 本来データが存在するはずなのにサンプリングできていない（データを取得できていない）空間があると、 その部分の予測精度は落ちてしまう。これについて少し触れてみたい。]]></summary></entry><entry><title type="html">因子分析と主成分分析</title><link href="https://keisuke-yanagisawa.github.io/202201/factor-analysis-and-principal-component-analysis/" rel="alternate" type="text/html" title="因子分析と主成分分析" /><published>2022-01-30T00:00:00+00:00</published><updated>2022-01-30T00:00:00+00:00</updated><id>https://keisuke-yanagisawa.github.io/202201/factor-analysis-and-principal-component-analysis</id><content type="html" xml:base="https://keisuke-yanagisawa.github.io/202201/factor-analysis-and-principal-component-analysis/"><![CDATA[<p>因子分析と主成分分析は似通った手法のように見える。
しかし、実際には大きく異なる点がある。</p>

<ul>
  <li><strong>因子分析</strong>：人間が「こういう組み合わせが共通因子として存在するのではないか」と考えて因子負荷量を決める
    <ul>
      <li>人間が考えながら因子を作り出すので、解釈性の高い結果を作りだせる。他人への説明が容易。</li>
    </ul>
  </li>
  <li><strong>主成分分析</strong>：「データに基づいて共通因子（＝主成分）を探してみる」方法で、因子負荷量はデータから自動的に決まる
    <ul>
      <li>自動的に作成されるので、得られた因子負荷量から、その因子が何を意味しているのかを人間が検討する。新しい発見につながる可能性がある。</li>
    </ul>
  </li>
</ul>

<p>ようするに、目的が異なるのである。</p>

<h5 id="参考文献">参考文献</h5>
<ul>
  <li>江崎貴裕『分析者のためのデータ解釈学入門　データの本質をとらえる技術』（ソシム、2020年）</li>
</ul>]]></content><author><name>Keisuke Yanagisawa</name><email>yanagisawa@c.titech.ac.jp</email></author><category term="データサイエンス・機械学習" /><category term="因子分析" /><category term="主成分分析" /><category term="教師なし学習" /><summary type="html"><![CDATA[因子分析と主成分分析は似通った手法のように見える。 しかし、実際には大きく異なる点がある。]]></summary></entry></feed>